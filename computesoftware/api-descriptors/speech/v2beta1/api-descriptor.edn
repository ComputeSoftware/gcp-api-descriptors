#:compute.gcp.descriptor{:name "speech", :title "Cloud Speech-to-Text API", :api-version "v2beta1", :revision "20200409", :endpoint #:compute.gcp.descriptor{:url "https://speech.googleapis.com/", :batch-path "batch", :service-path ""}, :parameters {"callback" {"description" "JSONP", "type" "string", "location" "query"}, "uploadType" {"location" "query", "description" "Legacy upload protocol for media (e.g. \"media\", \"multipart\").", "type" "string"}, "key" {"type" "string", "location" "query", "description" "API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token."}, "access_token" {"type" "string", "location" "query", "description" "OAuth access token."}, "oauth_token" {"location" "query", "description" "OAuth 2.0 token for the current user.", "type" "string"}, "prettyPrint" {"type" "boolean", "default" "true", "location" "query", "description" "Returns response with indentations and line breaks."}, "alt" {"enum" ["json" "media" "proto"], "type" "string", "enumDescriptions" ["Responses with Content-Type of application/json" "Media download with context-dependent Content-Type" "Responses with Content-Type of application/x-protobuf"], "location" "query", "description" "Data format for response.", "default" "json"}, "$.xgafv" {"type" "string", "enumDescriptions" ["v1 error format" "v2 error format"], "location" "query", "enum" ["1" "2"], "description" "V1 error format."}, "fields" {"type" "string", "location" "query", "description" "Selector specifying which fields to include in a partial response."}, "upload_protocol" {"description" "Upload protocol for media (e.g. \"raw\", \"multipart\").", "type" "string", "location" "query"}, "quotaUser" {"description" "Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.", "type" "string", "location" "query"}}, :op->info {}, :schemas {"WordInfo" {"type" "object", "properties" {"word" {"description" "Output only. The word corresponding to this set of information.", "type" "string"}, "speakerTag" {"description" "Output only. A distinct integer value is assigned for every speaker within\nthe audio. This field specifies which one of those speakers was detected to\nhave spoken this word. Value ranges from `1` to\n`diarization_config.max_speaker_count` . `speaker_tag` is set if\n`diarization_config.enable_speaker_diarization` = `true` and only in the\ntop alternative.", "format" "int32", "type" "integer"}, "endOffset" {"description" "Output only. Time offset relative to the beginning of the audio,\nand corresponding to the end of the spoken word.\nThis field is only set if `enable_word_time_offsets=true` and only\nin the top hypothesis.\nThis is an experimental feature and the accuracy of the time offset can\nvary.", "format" "google-duration", "type" "string"}, "confidence" {"description" "Output only. The confidence estimate between 0.0 and 1.0. A higher number\nindicates an estimated greater likelihood that the recognized words are\ncorrect. This field is set only for the top alternative of a non-streaming\nresult or, of a streaming result where `is_final=true`.\nThis field is not guaranteed to be accurate and users should not rely on it\nto be always provided.\nThe default of 0.0 is a sentinel value indicating `confidence` was not set.", "format" "float", "type" "number"}, "startOffset" {"type" "string", "description" "Output only. Time offset relative to the beginning of the audio,\nand corresponding to the start of the spoken word.\nThis field is only set if `enable_word_time_offsets=true` and only\nin the top hypothesis.\nThis is an experimental feature and the accuracy of the time offset can\nvary.", "format" "google-duration"}}, "id" "WordInfo", "description" "Word-specific information for recognized words."}, "LongRunningRecognizeMetadata" {"id" "LongRunningRecognizeMetadata", "description" "Describes the progress of a long-running `LongRunningRecognize` call. It is\nincluded in the `metadata` field of the `Operation` returned by the\n`GetOperation` call of the `google::longrunning::Operations` service.", "type" "object", "properties" {"uri" {"description" "The URI of the audio file being transcribed. Empty if the audio was sent\nas byte content.", "type" "string"}, "lastUpdateTime" {"type" "string", "description" "Output only. Time of the most recent processing update.", "format" "google-datetime"}, "progressPercent" {"description" "Output only. Approximate percentage of audio processed thus far. Guaranteed to be 100\nwhen the audio is fully processed and the results are available.", "format" "int32", "type" "integer"}, "startTime" {"type" "string", "description" "Output only. Time when the request was received.", "format" "google-datetime"}}}, "Status" {"type" "object", "properties" {"message" {"type" "string", "description" "A developer-facing error message, which should be in English. Any\nuser-facing error message should be localized and sent in the\ngoogle.rpc.Status.details field, or localized by the client."}, "details" {"description" "A list of messages that carry the error details.  There is a common set of\nmessage types for APIs to use.", "type" "array", "items" {"type" "object", "additionalProperties" {"description" "Properties of the object. Contains field @type with type URL.", "type" "any"}}}, "code" {"type" "integer", "description" "The status code, which should be an enum value of google.rpc.Code.", "format" "int32"}}, "id" "Status", "description" "The `Status` type defines a logical error model that is suitable for\ndifferent programming environments, including REST APIs and RPC APIs. It is\nused by [gRPC](https://github.com/grpc). Each `Status` message contains\nthree pieces of data: error code, error message, and error details.\n\nYou can find out more about this error model and how to work with it in the\n[API Design Guide](https://cloud.google.com/apis/design/errors)."}, "LongRunningRecognizeResponse" {"description" "The only message returned to the client by the `LongRunningRecognize` method.\nIt contains the result as zero or more sequential SpeechRecognitionResult\nmessages. It is included in the `result.response` field of the `Operation`\nreturned by the `GetOperation` call of the `google::longrunning::Operations`\nservice.", "type" "object", "properties" {"results" {"type" "array", "items" {"$ref" "SpeechRecognitionResult"}, "description" "Output only. Sequential list of transcription results corresponding to\nsequential portions of audio."}}, "id" "LongRunningRecognizeResponse"}, "Operation" {"type" "object", "properties" {"response" {"description" "The normal response of the operation in case of success.  If the original\nmethod returns no data on success, such as `Delete`, the response is\n`google.protobuf.Empty`.  If the original method is standard\n`Get`/`Create`/`Update`, the response should be the resource.  For other\nmethods, the response should have the type `XxxResponse`, where `Xxx`\nis the original method name.  For example, if the original method name\nis `TakeSnapshot()`, the inferred response type is\n`TakeSnapshotResponse`.", "type" "object", "additionalProperties" {"description" "Properties of the object. Contains field @type with type URL.", "type" "any"}}, "name" {"description" "The server-assigned name, which is only unique within the same service that\noriginally returns it. If you use the default HTTP mapping, the\n`name` should be a resource name ending with `operations/{unique_id}`.", "type" "string"}, "error" {"description" "The error result of the operation in case of failure or cancellation.", "$ref" "Status"}, "metadata" {"type" "object", "additionalProperties" {"description" "Properties of the object. Contains field @type with type URL.", "type" "any"}, "description" "Service-specific metadata associated with the operation.  It typically\ncontains progress information and common metadata such as create time.\nSome services might not provide such metadata.  Any method that returns a\nlong-running operation should document the metadata type, if any."}, "done" {"description" "If the value is `false`, it means the operation is still in progress.\nIf `true`, the operation is completed, and either `error` or `response` is\navailable.", "type" "boolean"}}, "id" "Operation", "description" "This resource represents a long-running operation that is the result of a\nnetwork API call."}, "ListOperationsResponse" {"description" "The response message for Operations.ListOperations.", "type" "object", "properties" {"nextPageToken" {"description" "The standard List next-page token.", "type" "string"}, "operations" {"description" "A list of operations that matches the specified filter in the request.", "type" "array", "items" {"$ref" "Operation"}}}, "id" "ListOperationsResponse"}, "SpeechRecognitionAlternative" {"description" "Alternative hypotheses (a.k.a. n-best list).", "type" "object", "properties" {"confidence" {"description" "Output only. The confidence estimate between 0.0 and 1.0. A higher number\nindicates an estimated greater likelihood that the recognized words are\ncorrect. This field is set only for the top alternative of a non-streaming\nresult or, of a streaming result where `is_final=true`.\nThis field is not guaranteed to be accurate and users should not rely on it\nto be always provided.\nThe default of 0.0 is a sentinel value indicating `confidence` was not set.", "format" "float", "type" "number"}, "transcript" {"description" "Output only. Transcript text representing the words that the user spoke.", "type" "string"}, "words" {"description" "Output only. A list of word-specific information for each recognized word.\nNote: When `enable_speaker_diarization` is true, you will see all the words\nfrom the beginning of the audio.", "type" "array", "items" {"$ref" "WordInfo"}}}, "id" "SpeechRecognitionAlternative"}, "SpeechRecognitionResult" {"id" "SpeechRecognitionResult", "description" "A speech recognition result corresponding to a portion of the audio.", "type" "object", "properties" {"alternatives" {"description" "Output only. May contain one or more recognition hypotheses (up to the\nmaximum specified in `max_alternatives`).\nThese alternatives are ordered in terms of accuracy, with the top (first)\nalternative being the most probable, as ranked by the recognizer.", "type" "array", "items" {"$ref" "SpeechRecognitionAlternative"}}, "channelTag" {"description" "Output only. For multi-channel audio, this is the channel number corresponding to the\nrecognized result for the audio from that channel.\nFor `audio_channel_count` = N, its output values can range from `1` to `N`.", "format" "int32", "type" "integer"}, "languageCode" {"description" "Output only. The\n[BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the\nlanguage in this result. This language code was detected to have the most\nlikelihood of being spoken in the audio.", "type" "string"}}}}}